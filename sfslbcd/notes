
0. Requirement

This client can connect with sfsrwsd, sfszrwsd (distributed with
lbfs), and sfslbsd.


1. LBFS Implementation

LBFS related code are in read.C and write.C.

Each cached file entry contains the name of the cached file (e->fn),
and name of a "previous" cached file (e->prevfn). The cached file
changes name everytime LBFS need to fetch content of the file from
server. e->prevfn is only used while LBFS is fetching content from the
server: it points to the last file fetched from server. After a fetch
is completed, e->prevfn is set to e->fn. We need e->prevfn so we can
still use chunks from the previous version of a file. 

It's not as important that we keep the previous version of a file when
we modify the file, because the application will write out a new
version, which, in scenarios where LBFS win, is similar to the older
version. When we write out the file to the server, we update the chunk
database with new chunk information.


2. Caching Client Implementation

2.1 Consistency:

Close-to-open consistency is: on any close (CLOSE rpc), nfs client
flushes all changes to server; on any open (ACCESS rpc), nfs client
fetches latest attributes from server, and loads new data when the
cache time is less than mtime.

Summary of consistency rules:

 0) Don't update cache if cached file is dirty or being flushed
 1) If lease has not expired, and cache time matches mtime, use cache
 2) If lease has expired, fetch attribute; if cache time matches
    mtime, use cache
 3) If lease has expired, fetch attribute; if cache time does not
    match mtime, update cache

What happens between open and close of a file is not covered by
close-to-open consistency. For example, what should happen on a
synchronous write? What should happen if the caching client receives a
COMMIT from the in-kernel client?

In our implementation, the following happens:
 
  - A synchronous write or COMMIT causes the cached content to be
    flushed to server within N seconds (currently N=2).

  - On a CLOSE, the caching client always flushes cached content to
    server before replying.

This semantics differ from expected semantics in two ways. One, a
synchronous write or COMMIT does not cause the data to be stored
permanently on a server immediately. Two, content is not visible to
another client immediately after a synchronous write or COMMIT.

This means if an application crashes within N seconds of sending a
synchronous write or commit, content may be lost. To be safe, an
application should close the file then re-open it.

We may try to fix this problem in a later version of lbfs.


2.2 Operations on cached file:

A cached file can be in one of the following four states

 OPEN: file has not been fetched from server since last ACCESS RPC
 IDLE: file has been fetched from server at some point
DIRTY: dirty, not flushed backed to server yet
FLUSH: in the middle of being flushed to server
FETCH: in the middle of being fetched from server

Below is the state transition rule when different RPC occurs. X means
exception. That is, the cached file should not be in this state when
this RPC occurs. (B) means the RPC will be blocked and executed after
the cached file changes state again.

       ACCESS  FETCH_DONE    READ     WRITE   SETATTR  CLOSE  FLUSH_DONE
------------------------------------------------------------------------
 OPEN    OPEN      X       FETCH(B)  FETCH(B)  OPEN     OPEN      X
FETCH   FETCH    IDLE        (B)       (B)      (B)     FETCH     X
 IDLE    OPEN      X        IDLE      DIRTY    IDLE     IDLE      X
DIRTY   DIRTY      X        DIRTY     DIRTY    DIRTY    FLUSH     X
FLUSH   FLUSH      X        FLUSH      (B)      (B)      (B)    IDLE


2.3 Implementation Details

sfslbcd keeps a negative lookup cache per directory accessed. Cache is
updated on CREATE, REMOVE, MKDIR, etc. Cache is invalidated when we
receive an INVALIDATE from server on the directory or when directory
mtime changed by another client.

Similarly, it also keeps a lookup cache. This lookup cache is more
aggresive than the one in kernel because it knows if a directory is
modified by another client or not.

Fetch optimization: after each data block fetched from server, check
if any one of the read or write RPC can be executed. A RPC can run if
it operates on a range of the data already received from the server.

Attribute cache code is shared between sfslbcd and sfsrwcd. For
sfslbcd we keep our own file cache, and each file cache entry has an
attribute. Attribute cache always reflects what the server knows about
a file; whereas attributes from the file cache may reflect changes yet
to be flushed to the server.

When a file is dirty or being flushed, GETATTR uses attribute from
file cache. In fact, we intercept ACCESS, GETATTR, SETATTR, and LOOKUP
replies and change the size attribute to that in the file cache.

On WRITE: on each write, update the size in file cache. On CLOSE: send
WRITE with UNSTABLE, then send COMMIT. On SETATTR: if size is changed,
update size in file cache. Forwards setattr to server so other
attributes are changed on server. We copy the attributes returned from
server to the file cache, but w/o replacing the size and mtime fields.

WCC: wcc checking is done whenever we modify a file on server, via
WRITE, SETATTR, or COMMIT. The goal of doing wcc checking is to avoid
doing a file fetch on the next open if only one client is modifying
the file. Because changes in ctime will not trigger a cache update, we
only check that the before and after size and mtime are the same, but
not ctime.

To make wcc work, "osize" field is kept for each cached object that
reflects the size of the object on server. As the size of the object
in cache changes, osize does not change. osize is updated when the
cached object is updated, flushed, or when setattr is processed (since
setattr is always forwarded to server)

Note, we may not return correct wcc information to the in-kernel nfs
client, because writes to the server are delayed. Thus, on the first
open after a close that modified the content on the server, the
in-kernel nfs client will ask the lbfs caching client for data, even
though it is the only client that wrote to the file. The lbfs caching
client will return content from its cache, but it won't need to
contact the server.

